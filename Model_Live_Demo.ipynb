{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97f37f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Torch: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "Torch CUDA build: 12.1\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "print('Python:', sys.version)\n",
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('Torch CUDA build:', getattr(torch.version, 'cuda', None))\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    print('CUDA device count:', torch.cuda.device_count())\n",
    "else:\n",
    "    print('NOTE: CUDA not available in this kernel.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdb80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\CVDL\\cvdl assign\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from transformers import AutoImageProcessor, BeitForImageClassification\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "from pathlib import Path\n",
    "model_path = str(Path(r\"Model_Path\").resolve())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce408940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path (repr): 'D:\\\\Programs\\\\CVDL\\\\cvdl assign\\\\final model 4'\n",
      "exists: True\n",
      "resolved: D:\\Programs\\CVDL\\cvdl assign\\final model 4\n"
     ]
    }
   ],
   "source": [
    "# Debug: show the exact contents and whether path exists\n",
    "print(\"model_path (repr):\", repr(model_path))\n",
    "import os\n",
    "print(\"exists:\", os.path.exists(model_path))\n",
    "from pathlib import Path\n",
    "try:\n",
    "    print(\"resolved:\", Path(model_path).resolve())\n",
    "except Exception as e:\n",
    "    print(\"Path resolve error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dd66aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with labels: {0: 'bored', 1: 'confused', 2: 'engaged', 3: 'neutral'}\n",
      "Face detector initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\CVDL\\cvdl assign\\.venv\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\Programs\\CVDL\\cvdl assign\\.venv\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\Programs\\CVDL\\cvdl assign\\.venv\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n"
     ]
    }
   ],
   "source": [
    "model = BeitForImageClassification.from_pretrained(model_path).to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "label2id = model.config.label2id\n",
    "print(f\"Loaded model with labels: {id2label}\")\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True, device=device, post_process=False)\n",
    "print(\"Face detector initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9202b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_faces(image):\n",
    "    \"\"\"\n",
    "    Detect faces in image and return cropped face regions with bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "    \n",
    "    Returns:\n",
    "        faces: List of cropped face PIL Images\n",
    "        boxes: List of bounding boxes [x1, y1, x2, y2]\n",
    "        landmarks: List of facial landmarks\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image_np = image\n",
    "    \n",
    "    boxes, probs, landmarks = mtcnn.detect(image_np, landmarks=True)\n",
    "    \n",
    "    if boxes is None:\n",
    "        return [], [], []\n",
    "    \n",
    "    faces = []\n",
    "    valid_boxes = []\n",
    "    valid_landmarks = []\n",
    "    \n",
    "    for i, (box, landmark) in enumerate(zip(boxes, landmarks)):\n",
    "        if probs[i] < 0.9:\n",
    "            continue\n",
    "            \n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        margin = int((x2 - x1) * 0.2)\n",
    "        \n",
    "        x1 = max(0, x1 - margin)\n",
    "        y1 = max(0, y1 - margin)\n",
    "        x2 = min(image_np.shape[1], x2 + margin)\n",
    "        y2 = min(image_np.shape[0], y2 + margin)\n",
    "        \n",
    "        face_crop = image_np[y1:y2, x1:x2]\n",
    "        faces.append(Image.fromarray(face_crop))\n",
    "        valid_boxes.append([x1, y1, x2, y2])\n",
    "        valid_landmarks.append(landmark)\n",
    "    \n",
    "    return faces, valid_boxes, valid_landmarks\n",
    "\n",
    "def align_face(image, landmarks):\n",
    "    \"\"\"\n",
    "    Align face using eye positions\n",
    "    \"\"\"\n",
    "    if landmarks is None:\n",
    "        return image\n",
    "    \n",
    "    left_eye = landmarks[0]\n",
    "    right_eye = landmarks[1]\n",
    "    \n",
    "    dY = right_eye[1] - left_eye[1]\n",
    "    dX = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "    \n",
    "    image_np = np.array(image)\n",
    "    center = tuple(np.array(image_np.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    aligned = cv2.warpAffine(image_np, rot_mat, image_np.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return Image.fromarray(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed59a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM initialized\n"
     ]
    }
   ],
   "source": [
    "class BeitGradCAM:\n",
    "    \"\"\"Grad-CAM wrapper for BEiT model\"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer=None):\n",
    "        self.model = model\n",
    "\n",
    "        if target_layer is None:\n",
    "            target_layer = model.beit.encoder.layer[-1].output\n",
    "\n",
    "        self.grad_cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM heatmap\n",
    "        \n",
    "        Args:\n",
    "            input_tensor: Preprocessed image tensor [1, 3, 224, 224]\n",
    "            target_class: Target class index (if None, uses predicted class)\n",
    "        \n",
    "        Returns:\n",
    "            cam: Grad-CAM heatmap\n",
    "            prediction: Model prediction\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_tensor)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            pred_class = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = pred_class\n",
    "        \n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        \n",
    "        cam = self.grad_cam(input_tensor=input_tensor, targets=targets)\n",
    "        \n",
    "        return cam[0], pred_class, probs[0].cpu().numpy()\n",
    "\n",
    "def visualize_gradcam(image, cam, pred_label, confidence, true_label=None):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM overlay on image\n",
    "    \"\"\"\n",
    "    image_resized = image.resize((224, 224))\n",
    "    image_np = np.array(image_resized).astype(np.float32) / 255.0\n",
    "    \n",
    "    visualization = show_cam_on_image(image_np, cam, use_rgb=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Face\", fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title(\"Attention Heatmap\", fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    title = f\"Prediction: {pred_label}\\nConfidence: {confidence:.2%}\"\n",
    "    if true_label:\n",
    "        title += f\"\\nTrue: {true_label}\"\n",
    "    axes[2].imshow(visualization)\n",
    "    axes[2].set_title(title, fontsize=12)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "grad_cam_generator = BeitGradCAM(model)\n",
    "print(\"Grad-CAM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c041eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalSmoother:\n",
    "    \"\"\"Smooth predictions over time using exponential moving average\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, window_size=10, alpha=0.3):\n",
    "        self.num_classes = num_classes\n",
    "        self.window_size = window_size\n",
    "        self.alpha = alpha\n",
    "        self.history = deque(maxlen=window_size)\n",
    "        self.ema_probs = None\n",
    "    \n",
    "    def update(self, probs):\n",
    "        \"\"\"\n",
    "        Update with new probabilities\n",
    "        \n",
    "        Args:\n",
    "            probs: numpy array of class probabilities [num_classes]\n",
    "        \n",
    "        Returns:\n",
    "            smoothed_probs: Smoothed probabilities\n",
    "        \"\"\"\n",
    "        self.history.append(probs)\n",
    "        \n",
    "        if self.ema_probs is None:\n",
    "            self.ema_probs = probs\n",
    "        else:\n",
    "            self.ema_probs = self.alpha * probs + (1 - self.alpha) * self.ema_probs\n",
    "        \n",
    "        return self.ema_probs\n",
    "    \n",
    "    def get_smoothed(self):\n",
    "        \"\"\"Get current smoothed probabilities\"\"\"\n",
    "        return self.ema_probs if self.ema_probs is not None else np.zeros(self.num_classes)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset history\"\"\"\n",
    "        self.history.clear()\n",
    "        self.ema_probs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fe1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(face_image, use_gradcam=False):\n",
    "    \"\"\"\n",
    "    Predict emotion for a single face image\n",
    "    \n",
    "    Args:\n",
    "        face_image: PIL Image of face\n",
    "        use_gradcam: Whether to generate Grad-CAM visualization\n",
    "    \n",
    "    Returns:\n",
    "        prediction_dict: Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    inputs = processor(images=face_image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred_class = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    probs_np = probs[0].cpu().numpy()\n",
    "    pred_label = id2label[pred_class]\n",
    "    confidence = probs_np[pred_class]\n",
    "    \n",
    "    result = {\n",
    "        'label': pred_label,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {id2label[i]: float(probs_np[i]) for i in range(len(probs_np))},\n",
    "        'class_id': pred_class\n",
    "    }\n",
    "    \n",
    "    if use_gradcam:\n",
    "        cam, _, _ = grad_cam_generator.generate_cam(pixel_values, pred_class)\n",
    "        result['gradcam'] = cam\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221fbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_file(image_path, use_gradcam=True):\n",
    "    \"\"\"\n",
    "    Analyze a single image file with all features\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    print(f\"Analyzing: {os.path.basename(image_path)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    faces, boxes, landmarks = detect_and_crop_faces(image)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected in image\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Detected {len(faces)} face(s)\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for i, (face, box, landmark) in enumerate(zip(faces, boxes, landmarks)):\n",
    "        print(f\"Face {i+1}:\")\n",
    "        \n",
    "        aligned_face = align_face(face, landmark)\n",
    "        \n",
    "        result = predict_emotion(aligned_face, use_gradcam=use_gradcam)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Prediction: {result['label']} ({result['confidence']:.2%})\")\n",
    "        print(f\"\\n  All Probabilities:\")\n",
    "        for emotion, prob in sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True):\n",
    "            bar = \"█\" * int(prob * 30)\n",
    "            print(f\"    {emotion:12s}: {bar} {prob:.2%}\")\n",
    "        \n",
    "        if use_gradcam:\n",
    "            fig = visualize_gradcam(\n",
    "                aligned_face,\n",
    "                result['gradcam'],\n",
    "                result['label'],\n",
    "                result['confidence']\n",
    "            )\n",
    "            plt.show()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    image_np = np.array(image)\n",
    "    for i, (box, result) in enumerate(zip(boxes, results)):\n",
    "        x1, y1, x2, y2 = box\n",
    "        color = (0, 255, 0) if result['confidence'] > 0.7 else (255, 165, 0)\n",
    "        cv2.rectangle(image_np, (x1, y1), (x2, y2), color, 3)\n",
    "        \n",
    "        label = f\"{i+1}: {result['label']} {result['confidence']:.2%}\"\n",
    "        cv2.putText(image_np, label, (x1, y1 - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Face Detection + Emotion Recognition Results\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56a85f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENHANCED EMOTION RECOGNITION SYSTEM READY!\n",
      "============================================================\n",
      "\n",
      "Available Functions:\n",
      "\n",
      "1. analyze_image_file('path/to/image.jpg')\n",
      "   - Analyze image file with all features\n",
      "\n",
      "2. detect_and_crop_faces(image)\n",
      "   - Face detection only\n",
      "\n",
      "3. predict_emotion(face_image, use_gradcam=True)\n",
      "   - Emotion prediction with optional Grad-CAM\n",
      "============================================================\n",
      "\n",
      "Example Usage:\n",
      "# Analyze an image:\n",
      "analyze_image_file('/path/to/your/image.jpg')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED EMOTION RECOGNITION SYSTEM READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable Functions:\\n\")\n",
    "print(\"1. analyze_image_file('path/to/image.jpg')\")\n",
    "print(\"   - Analyze image file with all features\")\n",
    "print(\"\\n2. detect_and_crop_faces(image)\")\n",
    "print(\"   - Face detection only\")\n",
    "print(\"\\n3. predict_emotion(face_image, use_gradcam=True)\")\n",
    "print(\"   - Emotion prediction with optional Grad-CAM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nExample Usage:\")\n",
    "print(\"# Analyze an image:\")\n",
    "print(\"analyze_image_file('/path/to/your/image.jpg')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba04c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_demo_single_capture():\n",
    "    \"\"\"\n",
    "    Single webcam capture with complete analysis:\n",
    "    - Face detection & alignment\n",
    "    - Emotion prediction with confidence\n",
    "    - Grad-CAM visualization\n",
    "    - All emotion probabilities displayed\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not access local webcam.\")\n",
    "        print(\"Ensure a webcam is connected and not used by another app\")\n",
    "        return\n",
    "    \n",
    "    print(\"Capturing image in 2 seconds...\")\n",
    "    print(\"Look at the camera and make an expression!\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < 2:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imshow('Webcam - Get Ready!', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame from webcam\")\n",
    "        return\n",
    "        \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    print(\"Image captured! Analyzing...\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    faces, boxes, landmarks = detect_and_crop_faces(image)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected!\")\n",
    "        print(\"Try again with better lighting or camera position\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Captured Image (No Face Detected)\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    print(f\"Detected {len(faces)} face(s)\\n\")\n",
    "    \n",
    "    for face_idx, (face, box, landmark) in enumerate(zip(faces, boxes, landmarks)):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FACE #{face_idx + 1} ANALYSIS\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        if not isinstance(face, Image.Image):\n",
    "            if isinstance(face, str):\n",
    "                face = Image.open(face).convert('RGB')\n",
    "            else:\n",
    "                face = Image.fromarray(face.astype('uint8'))\n",
    "                \n",
    "        aligned_face = align_face(face, landmark)\n",
    "        if not isinstance(aligned_face, Image.Image):\n",
    "            aligned_face = Image.fromarray(aligned_face.astype('uint8'))\n",
    "        aligned_face = aligned_face.convert('RGB')\n",
    "        \n",
    "        result = predict_emotion(aligned_face, use_gradcam=True)\n",
    "        \n",
    "        print(f\"EMOTION PREDICTION\")\n",
    "        print(f\"   Primary Emotion: {result['label'].upper()}\")\n",
    "        print(f\"   Confidence: {result['confidence']:.1%}\")\n",
    "        \n",
    "        if result['confidence'] > 0.8:\n",
    "            print(f\"   Assessment: Very Confident\")\n",
    "        elif result['confidence'] > 0.6:\n",
    "            print(f\"   Assessment: Confident\")\n",
    "        else:\n",
    "            print(f\"   Assessment: Uncertain\")\n",
    "        \n",
    "        print(f\"\\nALL EMOTION PROBABILITIES:\")\n",
    "        print(f\"   {'-'*50}\")\n",
    "        \n",
    "        sorted_emotions = sorted(\n",
    "            result['probabilities'].items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for emotion, prob in sorted_emotions:\n",
    "            bar_length = int(prob * 40)\n",
    "            bar = \"█\" * bar_length\n",
    "            bar_empty = \"░\" * (40 - bar_length)\n",
    "            \n",
    "            if emotion == result['label']:\n",
    "                marker = \">\"\n",
    "            else:\n",
    "                marker = \" \"\n",
    "            \n",
    "            print(f\"   {marker} {emotion:12s} |{bar}{bar_empty}| {prob:6.2%}\")\n",
    "        \n",
    "        print(f\"   {'-'*50}\\n\")\n",
    "        \n",
    "        if 'gradcam' in result and result['gradcam'] is not None:\n",
    "            print(\"GRAD-CAM ATTENTION VISUALIZATION:\\n\")\n",
    "            fig = visualize_gradcam(\n",
    "                aligned_face,\n",
    "                result['gradcam'],\n",
    "                result['label'],\n",
    "                result['confidence']\n",
    "            )\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Could not generate Grad-CAM visualization\\n\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"ANNOTATED RESULT\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    if isinstance(image, Image.Image):\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image_np = image.copy()\n",
    "    \n",
    "    for i, (box, landmark) in enumerate(zip(boxes, landmarks)):\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        face = faces[i]\n",
    "        if not isinstance(face, Image.Image):\n",
    "            face = Image.fromarray(face.astype('uint8'))\n",
    "        aligned = align_face(face, landmark)\n",
    "        result = predict_emotion(aligned, use_gradcam=False)\n",
    "        \n",
    "        if result['confidence'] > 0.7:\n",
    "            color = (0, 255, 0)\n",
    "        elif result['confidence'] > 0.5:\n",
    "            color = (255, 165, 0)\n",
    "        else:\n",
    "            color = (0, 0, 255)\n",
    "        \n",
    "        cv2.rectangle(image_np, (x1, y1), (x2, y2), color, 4)\n",
    "        \n",
    "        for point in landmark:\n",
    "            cv2.circle(image_np, tuple(point.astype(int)), 4, (255, 0, 255), -1)\n",
    "        \n",
    "        label = f\"#{i+1}: {result['label']} ({result['confidence']:.1%})\"\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1.0\n",
    "        thickness = 2\n",
    "        (text_w, text_h), _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "        \n",
    "        cv2.rectangle(image_np, (x1, y1 - text_h - 15), \n",
    "                     (x1 + text_w + 10, y1), color, -1)\n",
    "        \n",
    "        cv2.putText(image_np, label, (x1 + 5, y1 - 10),\n",
    "                   font, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Final Result: Face Detection + Emotion Recognition + Landmarks\", fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a184808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nQuick Start:\")\n",
    "# print(\"Running locally - using OpenCV webcam\")\n",
    "# live_demo_single_capture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73fec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_emotion_detection():\n",
    "    \"\"\"\n",
    "    Real-time emotion detection using webcam feed\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import time\n",
    "    \n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Could not access webcam\")\n",
    "        return\n",
    "    \n",
    "    cv2.namedWindow('Real-time Emotion Detection', cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    smoother = TemporalSmoother(num_classes=len(id2label), window_size=10)\n",
    "    \n",
    "    print(\"Starting real-time emotion detection...\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame\")\n",
    "            break\n",
    "            \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        faces, boxes, _ = detect_and_crop_faces(pil_image)\n",
    "        \n",
    "        for face, box in zip(faces, boxes):\n",
    "            if not isinstance(face, Image.Image):\n",
    "                face = Image.fromarray(face.astype('uint8'))\n",
    "            \n",
    "            result = predict_emotion(face, use_gradcam=False)\n",
    "            \n",
    "            probs_array = np.array([result['probabilities'][id2label[j]] for j in range(len(id2label))])\n",
    "            smoothed_probs = smoother.update(probs_array)\n",
    "            smoothed_class = np.argmax(smoothed_probs)\n",
    "            smoothed_label = id2label[smoothed_class]\n",
    "            smoothed_confidence = smoothed_probs[smoothed_class]\n",
    "            \n",
    "            x1, y1, x2, y2 = box\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            label = f\"{smoothed_label.upper()}: {smoothed_confidence:.1%}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), \n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        cv2.imshow('Real-time Emotion Detection', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nReal-time detection stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddfaa554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting real-time emotion detection...\")\n",
    "# print(\"Please allow camera access when prompted\")\n",
    "# print(\"Press 'q' to quit the application\\n\")\n",
    "\n",
    "# real_time_emotion_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b6a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen capture library installed and imported successfully!\n",
      "mss: Fast cross-platform screen capture\n"
     ]
    }
   ],
   "source": [
    "import mss\n",
    "\n",
    "print(\"Screen capture library installed and imported successfully!\")\n",
    "print(\"mss: Fast cross-platform screen capture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b82a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def screen_emotion_detection(monitor=1, fps=5, region=None, use_gradcam=False, use_temporal=True):\n",
    "    \"\"\"\n",
    "    Real-time emotion detection from screen capture using mss library\n",
    "    \n",
    "    Args:\n",
    "        monitor: Monitor number (1 = primary, 0 = all monitors)\n",
    "        fps: Frames per second for capture (default: 5)\n",
    "        region: Optional (x, y, width, height) to capture specific region\n",
    "        use_gradcam: Whether to show Grad-CAM visualization\n",
    "        use_temporal: Whether to use temporal smoothing\n",
    "    \n",
    "    Usage:\n",
    "        screen_emotion_detection()\n",
    "        screen_emotion_detection(region=(100, 100, 800, 600))\n",
    "        screen_emotion_detection(fps=10, use_gradcam=True)\n",
    "    \"\"\"\n",
    "    print(\"Starting screen emotion detection...\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if use_temporal:\n",
    "        smoother = TemporalSmoother(num_classes=len(id2label), window_size=10)\n",
    "    \n",
    "    with mss.mss() as sct:\n",
    "        if region:\n",
    "            x, y, w, h = region\n",
    "            monitor_dims = {\"top\": y, \"left\": x, \"width\": w, \"height\": h}\n",
    "            print(f\"Capturing region: {region}\")\n",
    "        else:\n",
    "            monitor_dims = sct.monitors[monitor]\n",
    "            print(f\"Capturing monitor {monitor}: {monitor_dims}\")\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            screenshot = sct.grab(monitor_dims)\n",
    "            \n",
    "            img_np = np.array(screenshot)\n",
    "            \n",
    "            img_rgb = cv2.cvtColor(img_np, cv2.COLOR_BGRA2RGB)\n",
    "            pil_image = Image.fromarray(img_rgb)\n",
    "            \n",
    "            faces, boxes, landmarks = detect_and_crop_faces(pil_image)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if len(faces) > 0:\n",
    "                print(f\"\\nFrame {frame_count} - Found {len(faces)} face(s)\")\n",
    "                \n",
    "                for i, (face, box, landmark) in enumerate(zip(faces, boxes, landmarks)):\n",
    "                    if not isinstance(face, Image.Image):\n",
    "                        face = Image.fromarray(face.astype('uint8'))\n",
    "                    aligned_face = align_face(face, landmark)\n",
    "                    \n",
    "                    result = predict_emotion(aligned_face, use_gradcam=use_gradcam)\n",
    "                    \n",
    "                    if use_temporal:\n",
    "                        probs_array = np.array([result['probabilities'][id2label[j]] \n",
    "                                               for j in range(len(id2label))])\n",
    "                        smoothed_probs = smoother.update(probs_array)\n",
    "                        smoothed_class = np.argmax(smoothed_probs)\n",
    "                        smoothed_label = id2label[smoothed_class]\n",
    "                        smoothed_confidence = smoothed_probs[smoothed_class]\n",
    "                        \n",
    "                        print(f\"  Face {i+1}: {smoothed_label} ({smoothed_confidence:.1%})\")\n",
    "                    else:\n",
    "                        print(f\"  Face {i+1}: {result['label']} ({result['confidence']:.1%})\")\n",
    "                    \n",
    "                    x1, y1, x2, y2 = box\n",
    "                    color = (0, 255, 0) if result['confidence'] > 0.7 else (255, 165, 0)\n",
    "                    cv2.rectangle(img_np, (x1, y1), (x2, y2), color, 2)\n",
    "                    \n",
    "                    if use_temporal:\n",
    "                        label = f\"{smoothed_label}: {smoothed_confidence:.1%}\"\n",
    "                    else:\n",
    "                        label = f\"{result['label']}: {result['confidence']:.1%}\"\n",
    "                    \n",
    "                    cv2.putText(img_np, label, (x1, y1-10),\n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                    \n",
    "                    if use_gradcam and 'gradcam' in result and result['gradcam'] is not None:\n",
    "                        fig = visualize_gradcam(\n",
    "                            aligned_face,\n",
    "                            result['gradcam']['heatmap'],\n",
    "                            result['label'],\n",
    "                            result['confidence']\n",
    "                        )\n",
    "                        plt.show()\n",
    "            \n",
    "            cv2.imshow('Screen Emotion Detection', img_np)\n",
    "            \n",
    "            if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nScreen capture stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccaa3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_screen_region():\n",
    "    \"\"\"\n",
    "    Interactive GUI to select screen region for capture\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (x, y, width, height) of selected region\n",
    "    \"\"\"\n",
    "    import tkinter as tk\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.attributes('-alpha', 0.3)\n",
    "    root.attributes('-topmost', True)\n",
    "    root.configure(bg='blue')\n",
    "    \n",
    "    screen_width = root.winfo_screenwidth()\n",
    "    screen_height = root.winfo_screenheight()\n",
    "    root.geometry(f\"{screen_width}x{screen_height}+0+0\")\n",
    "    \n",
    "    canvas = tk.Canvas(root, cursor=\"cross\", bg='blue')\n",
    "    canvas.pack(fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    rect = None\n",
    "    start_x = start_y = 0\n",
    "    selected_region = None\n",
    "    \n",
    "    def on_press(event):\n",
    "        nonlocal start_x, start_y, rect\n",
    "        start_x, start_y = event.x, event.y\n",
    "        rect = canvas.create_rectangle(start_x, start_y, start_x, start_y,\n",
    "                                      outline='red', width=3)\n",
    "    \n",
    "    def on_drag(event):\n",
    "        canvas.coords(rect, start_x, start_y, event.x, event.y)\n",
    "    \n",
    "    def on_release(event):\n",
    "        nonlocal selected_region\n",
    "        end_x, end_y = event.x, event.y\n",
    "        selected_region = (min(start_x, end_x), min(start_y, end_y),\n",
    "                          abs(end_x - start_x), abs(end_y - start_y))\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "    \n",
    "    canvas.bind('<ButtonPress-1>', on_press)\n",
    "    canvas.bind('<B1-Motion>', on_drag)\n",
    "    canvas.bind('<ButtonRelease-1>', on_release)\n",
    "    \n",
    "    print(\"Drag to select screen region, then release\")\n",
    "    root.mainloop()\n",
    "    \n",
    "    return selected_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34bc4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"SCREEN EMOTION DETECTION DEMO\")\n",
    "# print(\"=\"*70)\n",
    "# print(\"\\nInstructions:\")\n",
    "# print(\"1. Open a video/image with faces on your screen\")\n",
    "# print(\"2. Uncomment ONE of the options below\")\n",
    "# print(\"3. Run this cell to start analysis\")\n",
    "# print(\"4. Press 'q' in the display window to stop\")\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# # OPTION 1: Full screen capture\n",
    "# screen_emotion_detection()\n",
    "\n",
    "# # OPTION 2: Select region first (recommended for video players)\n",
    "# # region = select_screen_region()\n",
    "# # if region:\n",
    "# #     print(f\"\\nSelected region: {region}\")\n",
    "# #     screen_emotion_detection(region=region, fps=5, use_gradcam=False, use_temporal=True)\n",
    "\n",
    "# # OPTION 3: Higher FPS with Grad-CAM\n",
    "# # screen_emotion_detection(fps=10, use_gradcam=True, use_temporal=True)\n",
    "\n",
    "# print(\"\\nUncomment one option above to start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ef9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage guidelines:\n",
      " - For options 1 and 2, ensure your webcam is connected and not in use by another app.\n",
      " - For option 3 (screen), make sure the screen content you want to analyze is visible and that the \"mss\" package is installed.\n",
      " - During real-time captures, press 'q' in the display window to stop the capture.\n",
      " - If running inside certain notebook environments where stdin is not available, the interactive menu may not work; in that case, call the functions directly from a cell.\n",
      "\n",
      "Demo Menu:\n",
      "  1 - Single webcam capture (live_demo_single_capture)\n",
      "  2 - Real-time webcam emotion detection (real_time_emotion_detection)\n",
      "  3 - Screen emotion detection (screen_emotion_detection)\n",
      "  q - Quit menu\n",
      "\n",
      "Running live_demo_single_capture()...\n",
      "\n",
      "Running live_demo_single_capture()...\n",
      "Capturing image in 2 seconds...\n",
      "Look at the camera and make an expression!\n",
      "Capturing image in 2 seconds...\n",
      "Look at the camera and make an expression!\n",
      "Image captured! Analyzing...\n",
      "\n",
      "======================================================================\n",
      "Image captured! Analyzing...\n",
      "\n",
      "======================================================================\n",
      "Detected 1 face(s)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FACE #1 ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Detected 1 face(s)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FACE #1 ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Error while running live_demo_single_capture(): 'str' object has no attribute 'shape'\n",
      "Error while running live_demo_single_capture(): 'str' object has no attribute 'shape'\n",
      "\n",
      "Running real_time_emotion_detection()...\n",
      "\n",
      "Running real_time_emotion_detection()...\n",
      "Starting real-time emotion detection...\n",
      "Press 'q' to quit\n",
      "Starting real-time emotion detection...\n",
      "Press 'q' to quit\n",
      "\n",
      "Real-time detection stopped\n",
      "\n",
      "Real-time detection stopped\n",
      "Exiting menu.\n",
      "Exiting menu.\n"
     ]
    }
   ],
   "source": [
    "def _show_menu_and_run():\n",
    "    \"\"\"Interactive menu to run demo functions:\\n    1 -> live_demo_single_capture\\n    2 -> real_time_emotion_detection\\n    3 -> screen_emotion_detection\\n    Enter 'q' to quit the menu.\"\"\"\n",
    "    print('\\nDemo Menu:')\n",
    "    print('  1 - Single webcam capture (live_demo_single_capture)')\n",
    "    print('  2 - Real-time webcam emotion detection (real_time_emotion_detection)')\n",
    "    print('  3 - Screen emotion detection (screen_emotion_detection)')\n",
    "    print(\"  q - Quit menu\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"\\nEnter 1, 2, 3 or q: \").strip()\n",
    "        except Exception as e:\n",
    "            # In environments without stdin, stop gracefully\n",
    "            print('Input not available in this environment:', e)\n",
    "            break\n",
    "        if choice == '1':\n",
    "            print('\\nRunning live_demo_single_capture()...')\n",
    "            try:\n",
    "                live_demo_single_capture()\n",
    "            except Exception as e:\n",
    "                print('Error while running live_demo_single_capture():', e)\n",
    "        elif choice == '2':\n",
    "            print('\\nRunning real_time_emotion_detection()...')\n",
    "            try:\n",
    "                real_time_emotion_detection()\n",
    "            except Exception as e:\n",
    "                print('Error while running real_time_emotion_detection():', e)\n",
    "        elif choice == '3':\n",
    "            print('\\nRunning screen_emotion_detection()...')\n",
    "            try:\n",
    "                screen_emotion_detection()\n",
    "            except Exception as e:\n",
    "                print('Error while running screen_emotion_detection():', e)\n",
    "        elif choice.lower() == 'q':\n",
    "            print('Exiting menu.')\n",
    "            break\n",
    "        else:\n",
    "            print('Invalid choice. Please enter 1, 2, 3 or q.')\n",
    "\n",
    "print('\\nUsage guidelines:')\n",
    "print(' - For options 1 and 2, ensure your webcam is connected and not in use by another app.')\n",
    "print(' - For option 3 (screen), make sure the screen content you want to analyze is visible and that the \"mss\" package is installed.')\n",
    "print(\" - During real-time captures, press 'q' in the display window to stop the capture.\")\n",
    "print(' - If running inside certain notebook environments where stdin is not available, the interactive menu may not work; in that case, call the functions directly from a cell.')\n",
    "# Run the interactive menu when this cell is executed\n",
    "_show_menu_and_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (cvdl assign)",
   "language": "python",
   "name": "cvdl-assign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
